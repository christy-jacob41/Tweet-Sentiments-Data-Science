install.packages(c("rtweet", "ggplot2", "dplyr","igraph", "ggraph", "rjson", "zoo"  ), dependencies = TRUE)
install.packages(c("rtweet", "ggplot2", "dplyr", "igraph", "ggraph", "rjson", "zoo"), dependencies = TRUE)
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# plotting packages
library(igraph)
library(ggraph)
library(rjson)
library(jsonlite)
library(zoo)
library(dplyr)
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
View(gw_tweets)
gw_tweets$stripped_text <- gsub("http.*","",  gw_tweets$text)
gw_tweets$stripped_text <- gsub("https.*","", gw_tweets$stripped_text)
gw_tweets_clean <- gw_tweets %>%
dplyr::select(stripped_text) %>%
unnest_tokens(word, stripped_text)
install.packages("sentimentr")
library(sentimentr)
?sentimentr
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
View(gw_tweets)
sentence_sentiment <- without_by(gw_tweets$text)
overall_sentiment <- sentiment_by(gw_tweets$text)
View(overall_sentiment)
qplot(overall_sentiment$ave_sentiment,geom="histogram",binwidth=0.2,main="Overall Sentiment")
summary(overall_sentiment$ave_sentiment)
sentence_senitment <- gw_tweets$text %>%
get_sentences() %>%
sentiment() %>%
mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
ifelse(sentiment > 0.2, "Positive","Neutral"
sentence_senitment <- gw_tweets$text %>%
get_sentences() %>%
sentiment() %>%
mutate(polarity_level = ifelse(sentiment < 0.2, "Negative",
ifelse(sentiment > 0.2, "Positive","Neutral")))
gw_tweets$text %>%
unnest %>%
sentimentr::get_sentences() %>%
sentimentr::sentiment() %>%
mutate(characters = nchar(stripWhitespace(text))) %>%
filter(characters >1 ) -> bounded_sentences
install.packages("tidyverse")
library(tidyverse)
gw_tweets$text %>%
unnest %>%
sentimentr::get_sentences() %>%
sentimentr::sentiment() %>%
mutate(characters = nchar(stripWhitespace(text))) %>%
filter(characters >1 ) -> bounded_sentences
gw_tweets <- search_tweets(q = "#globalwarming", n = 10000,
lang = "en",
include_rts = FALSE)
gw_tweets$stripped_text <- gsub("http.*","",  gw_tweets$text)
gw_tweets$stripped_text <- gsub("https.*","", gw_tweets$stripped_text)
sentences <- get_sentences(gw_tweets$text)
View(sentences)
sentences[0]
sentences[[1]]
sentences[[5000]]
sentences[[2000]]
sentiment_attributes(sentences[[1]])
sentiment_attributes(sentences[[1]])
# getting sentiment of sentence 1
sentiment(sentences[[1]])
# getting sentiment of sentences in tweet 1
sentences[[1]]
plot(overall_sentiment)
# getting the overall sentiment of all global warming tweets found
overall_sentiment <- sentiment_by(gw_tweets$text)
# summary statistics of the average sentiment of each tweet
summary(overall_sentiment$ave_sentiment)
# histogram of the average sentiment of each tweet
qplot(overall_sentiment$ave_sentiment,geom="histogram",binwidth=0.2,main="Overall Sentiment")
plot(overall_sentiment)
plot(overall_sentiment$word_count, overall_sentiment$ave_sentiment)
getSources()
VCorpus
?VCorpus
??VCorpus
install.packages("tm")
getSources()
getSources()
require(tm)
getSources()
docs<- c("UTD is great", "Students from great schools get great jobs", "I will get a great job")
vs <- VectorSource(docs)
vCorp <- VCorpus(vs)
vCorp <- tm_map(vCorp, stemDocument)
vCorp <- tm_map(vCorp, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(vCorp)
dim(tdm)
## ---- eval=TRUE, echo=FALSE, message = FALSE, error = FALSE, fig.height = 6.5----
require(quantmod)
require(tidyquant)
getSymbol("FB", from = Sys.Date() - 1 year)
getSymbol("FB", from = Sys.Date() - 365)
getSymbols("FB", from = Sys.Date() - 365)
getSymbols("FB", from = Sys.Date() - 1 year)
yearly_aapl_returns <- "AAPL" %>% tq_get(get = "stock.prices", to = Sys.Date()) %>% tq_transmute(select = adjusted, mutate_fun =
periodReturn, period = "yearly", col_rename = "Ra")
yearly_aapl_returns %>% mean(Ra), max(Ra), min(Ra), count()
yearly_aapl_returns %>% summarise(mean(Ra), max(Ra), min(Ra), n())
yearly_aapl_returns %>% summarize(mean(Ra), max(Ra), min(Ra), n())
require(tidyverse)
yearly_aapl_returns %>% summarize(mean(Ra), max(Ra), min(Ra), n())
yearly_aapl_returns %>% summarise(mean(Ra), max(Ra), min(Ra), n())
a <- tq_get("AAPL")
View(a)
type(a)
typeof(a)
getSymbols("AAPL")
head(a)
a
chartSeries(a, subset = '2019')
text <- c("utd is a great school", "students from a great school", "get great jobs", "i love utd")
text_df <- data_frame(line = 1:4, text = text)
f <- text_df %>% unnest_tokens(word, text)
library(dplyr)
require(tidytext)
f <- text_df %>% unnest_tokens(word, text)
f %>% group_by(word) %>% count()
f %>% group(word) %>% count()
f %>% count(word, sort=TRUE)
f <- text_df %>% unnest_tokens(word, text)
f %>% count(word, sort=TRUE)
text <- c("utd is a great school", "students from a great school", "get great jobs", "i love utd")
text_df <- data_frame(line = 1:4, text = text)
f <- text_df %>% unnest_tokens(word, text)
f %>% count(word, sort=TRUE)
text <- c("utd is a great school", "students from a great school", "get great jobs", "i love utd")
text_df <- data_frame(line = 1:4, text = text)
View(text_df)
f <- text_df %>% unnest_tokens(word, text)
View(f)
text <- c("UTD, is a great school", "students from a great school", "get great jobs", "i love utd")
text_df <- data_frame(line = 1:4, text = text)
View(text_df)
f <- text_df %>% unnest_tokens(word, text)
View(f)
text <- c("UTD, is a great school", "students from a great school", "get great jobs", "i love utd")
text_df <- data_frame(line = 1:4, text = text)
View(f)
a <- text_df %>% unnest_tokens(word, text)
View(a)
require(graphics)
## the variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
prcomp(~ Murder + Assault + Rape, data = USArrests, scale = TRUE)
## the variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
pc <- prcomp(~ Murder + Assault + Rape, data = USArrests, scale = TRUE)
summary(prcomp(USArrests))
prcomp(USArrests)$variance
summary(prcomp(USArrests))$x
pc <- prcomp(USArrests)$x
summary(prcomp(USArrests))
prcomp(USArrests)$variance
prcomp(USArrests)$varia
prcomp(USArrests)$proportion of variance
d <- data_frame((1,2))
d <- data_frame(c(1,2))
View(d)
d <- data_frame(c(c(1,2), c(3,4)))
View(d)
d <- data_frame(x= c(1,2), y= c(3,4)))
d <- data_frame(x= c(1,2), y= c(3,4))
View(d)
d <- data_frame(x= c(1,1.5,3,5,3.5,4.5,3.5), y= c(1,2,4,7,5,6,4.5))
k.means.fit <- kmeans(d, 3) # k = 3
## ----  results="hide"----------------------------------------------------
k.means.fit$cluster
k.means.fit$size
View(k.means.fit)
View(d)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
## ---- echo=FALSE---------------------------------------------------------
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
wssplot(d, nc=6)
sum(kmeans(d, centers=3)$withinss)
sum(kmeans(k.means.fit, centers=3)$withinss)
sum(kmeans(k.means.fit, centers=3))
wssplot(d, nc=3)
sum(kmeans(d, centers=3))
